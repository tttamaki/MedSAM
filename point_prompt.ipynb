{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "import gc\n",
    "from ipywidgets import widgets\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from segment_anything import sam_model_registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointPromptDemo:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.image = None\n",
    "        self.image_embeddings = None\n",
    "        self.img_size = None\n",
    "\n",
    "    def show_mask(self, mask, ax, random_color=False, alpha=0.95):\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n",
    "        else:\n",
    "            color = np.array([251 / 255, 252 / 255, 30 / 255, alpha])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape((1, 1, -1))\n",
    "        ax.imshow(mask_image)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, x, y):\n",
    "        coords_1024 = np.array([[[\n",
    "            x * 1024 / self.img_size[1],\n",
    "            y * 1024 / self.img_size[0]\n",
    "        ]]])\n",
    "        coords_torch = torch.tensor(coords_1024, dtype=torch.float32).to(self.model.device)\n",
    "        labels_torch = torch.tensor([[1]], dtype=torch.long).to(self.model.device)\n",
    "        point_prompt = (coords_torch, labels_torch)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points=point_prompt,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "        )\n",
    "        low_res_logits, _ = self.model.mask_decoder(\n",
    "            image_embeddings=self.image_embeddings,  # (B, 256, 64, 64)\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(),  # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings,  # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings,  # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        low_res_probs = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "        low_res_pred = F.interpolate(\n",
    "            low_res_probs,\n",
    "            size=self.img_size,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        low_res_pred = low_res_pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "        seg = np.uint8(low_res_pred > 0.5)\n",
    "\n",
    "        return seg\n",
    "\n",
    "    def show(self, fig_size=5, alpha=0.95, scatter_size=10):\n",
    "\n",
    "        assert self.image is not None, \"Please set image first.\"\n",
    "        seg = None\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(fig_size, fig_size))\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.canvas.footer_visible = False\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.canvas.resizable = False\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        ax.imshow(self.image)\n",
    "        ax.axis('off')\n",
    "\n",
    "        def onclick(event):\n",
    "            if event.inaxes == ax:\n",
    "                x, y = float(event.xdata), float(event.ydata)\n",
    "                with torch.no_grad():\n",
    "                    # rescale x, y from canvas size to 1024 x 1024\n",
    "                    seg = self.infer(x, y)\n",
    "\n",
    "                ax.clear()\n",
    "                ax.imshow(self.image)\n",
    "                ax.axis('off')\n",
    "                ax.scatter(x, y, c='r', s=scatter_size)\n",
    "                self.show_mask(seg, ax, random_color=False, alpha=alpha)\n",
    "\n",
    "                gc.collect()\n",
    "\n",
    "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "        plt.show()\n",
    "\n",
    "        save_button = widgets.Button(description=\"save\")\n",
    "\n",
    "        def __on_save_button_clicked(b):\n",
    "            plt.savefig(\"seg_result.png\", bbox_inches='tight', pad_inches=0)\n",
    "            if seg is not None:\n",
    "                cv2.imwrite(\"seg.png\", seg)\n",
    "                print(f\"Segmentation result saved to {getcwd()}\")\n",
    "\n",
    "        display(save_button)\n",
    "        save_button.on_click(__on_save_button_clicked)\n",
    "\n",
    "    def set_image(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        self.img_size = image.shape[:2]\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.repeat(image[:, :, None], 3, -1)\n",
    "        self.image = image\n",
    "        image_preprocess = self.preprocess_image(self.image)\n",
    "        with torch.no_grad():\n",
    "            self.image_embeddings= self.model.image_encoder(image_preprocess)\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        img_resize = cv2.resize(\n",
    "            image,\n",
    "            (1024, 1024),\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        # Resizing\n",
    "        # normalize to [0, 1], (H, W, 3\n",
    "        img_resize = (img_resize - img_resize.min()) / np.clip(\n",
    "            img_resize.max() - img_resize.min(), a_min=1e-8, a_max=None)\n",
    "        # convert the shape to (3, H, W)\n",
    "        assert (\n",
    "            np.max(img_resize) <= 1.0 and\n",
    "            np.min(img_resize) >= 0.0\n",
    "        ), 'image should be normalized to [0, 1]'\n",
    "        img_tensor=torch.tensor(img_resize).float().permute(\n",
    "            2, 0, 1).unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsam_ckpt_path = \"medsam_point_prompt_flare22.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=medsam_ckpt_path)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()\n",
    "point_prompt_demo = PointPromptDemo(medsam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = \"/mnt/SSD4TB/tamaki/20230125Endoscopy/dataset/20200730_00_fullset_endoscopy/0031547891_1_25_F0.jpg\"\n",
    "img = \"assets/img_demo.png\"\n",
    "point_prompt_demo.set_image(img)\n",
    "point_prompt_demo.show(alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
